{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e54d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "472c998c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load env variable\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a566fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/users/dhananjay/.cache/huggingface\n",
      "/data/users/dhananjay/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "hf_home_dir = os.environ['HF_HOME']\n",
    "transformer_dir = os.environ['TRANSFORMERS_CACHE']\n",
    "print(hf_home_dir)\n",
    "print(transformer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34acefc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# set cuda device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a4d94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset snli (/data/users/dhananjay/.cache/huggingface/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n",
      "Found cached dataset snli (/data/users/dhananjay/.cache/huggingface/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n",
      "Found cached dataset snli (/data/users/dhananjay/.cache/huggingface/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    }
   ],
   "source": [
    "# load english data\n",
    "train_ = load_dataset(\"snli\", split=\"train\", cache_dir = str(hf_home_dir))\n",
    "val_ = load_dataset(\"snli\", split=\"validation\", cache_dir = str(hf_home_dir))\n",
    "test_ = load_dataset(\"snli\", split=\"test\", cache_dir = str(hf_home_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04341b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_dataset(data, num_dataset):\n",
    "    if len(data) > num_dataset:\n",
    "        data['premise'] = data['premise'][:num_dataset]\n",
    "        data['hypothesis'] = data['premise'][:num_dataset]\n",
    "        data['label'] = data['premise'][:num_dataset]\n",
    "    assert len(data) == num_dataset\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4bb721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIEnglish(Dataset):\n",
    "    # data : Dataset --- input data file\n",
    "    # clip_dataset : int --- how many datapoints to be consider while converting to the french\n",
    "    #                        this is helpful if dataset is huge size and taking time for conversion to french\n",
    "    def __init__(self, data, clip_dataset):\n",
    "        self.data1 = self.clean_data(data)\n",
    "        self.clip_dataset = min(clip_dataset, len(self.data1))\n",
    "        \n",
    "    def clean_data(self, data):\n",
    "        return data.filter(lambda datapoint:datapoint['label'] != -1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.clip_dataset #len(self.data)\n",
    "\n",
    "    def __getitem__(self, n: int):\n",
    "        datapoint = self.data1[n]\n",
    "        task_prefix = \"translate English to French: \"\n",
    "        premise = task_prefix + datapoint['premise']\n",
    "        hypothesis = task_prefix + datapoint['hypothesis']\n",
    "        label = datapoint['label']\n",
    "        return premise, hypothesis, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09d97f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/users/dhananjay/.cache/huggingface/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-eba2ca8d6f1fbd02.arrow\n",
      "Loading cached processed dataset at /data/users/dhananjay/.cache/huggingface/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-ddf279f7e4ed9c36.arrow\n",
      "Loading cached processed dataset at /data/users/dhananjay/.cache/huggingface/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-56afeaebeebd3b97.arrow\n"
     ]
    }
   ],
   "source": [
    "english_train_dataset = SNLIEnglish(train_, 100000)\n",
    "english_val_dataset = SNLIEnglish(val_, len(val_))\n",
    "english_test_dataset = SNLIEnglish(test_, len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c14a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom collate function to batchify data\n",
    "def custom_collate_fn(batch):\n",
    "    premise, hypothesis, label = zip(*batch)\n",
    "    premise_enc = t5small_tokenizer.batch_encode_plus(premise, padding=True, return_tensors='pt')['input_ids'].to(device)\n",
    "    hypthesis_enc = t5small_tokenizer.batch_encode_plus(hypothesis, padding=True, return_tensors='pt')['input_ids'].to(device)\n",
    "    label_enc = torch.tensor(label).to(device)\n",
    "    return premise_enc, hypthesis_enc, label_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f9c41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate and save datafile locally to filepath\n",
    "def generate_french_data(dataloader, model, tokenizer, filepath):\n",
    "    french_data = defaultdict(list)\n",
    "    for premise_enc, hypthesis_enc, label_enc in tqdm(dataloader):\n",
    "        output_premise = model.generate(premise_enc, do_sample=False)\n",
    "        french_data['premise'].extend(tokenizer.batch_decode(output_premise, skip_special_tokens=True))\n",
    "        output_hypothesis = model.generate(hypthesis_enc, do_sample=False)\n",
    "        french_data['hypothesis'].extend(tokenizer.batch_decode(output_hypothesis, skip_special_tokens=True))\n",
    "        french_data['label'].extend(label_enc.tolist())\n",
    "    \n",
    "    hf_french_data = Dataset.from_dict(french_data)\n",
    "    with open(filepath, \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(hf_french_data, fp)\n",
    "    return hf_french_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce61229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_french_data(dataloader, model, tokenizer, filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"File doesn't exist :\", filepath)\n",
    "        print(\"creating file...\")\n",
    "        generate_french_data(dataloader, model, tokenizer, filepath)\n",
    "        print(\"File created!\")\n",
    "    with open(filepath, \"rb\") as fp:   #Pickling\n",
    "        hf_french_data = pickle.load(fp)\n",
    "    print(\"Successfully loaded file - \", filepath)\n",
    "    return hf_french_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6773108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "english_train_datloader = DataLoader(dataset = english_train_dataset, collate_fn = custom_collate_fn, batch_size = 512, shuffle = False)\n",
    "english_val_dataloader = DataLoader(dataset = english_val_dataset, collate_fn = custom_collate_fn, batch_size = 512, shuffle = False)\n",
    "english_test_dataloader = DataLoader(dataset = english_test_dataset, collate_fn = custom_collate_fn, batch_size = 512, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a12387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath to save intermediate files loacally\n",
    "curr_dir = os.getcwd()\n",
    "french_train_filepath = os.path.join(curr_dir, \"data\", \"french\", \"train\")\n",
    "french_validation_filepath = os.path.join(curr_dir, \"data\", \"french\", \"validation\")\n",
    "french_test_filepath = os.path.join(curr_dir, \"data\", \"french\", \"test\")\n",
    "final_filepath = os.path.join(curr_dir, \"data\", \"french\", \"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b70a8641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users/dhananjay/miniconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t5small_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "t5small_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fab0572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded file -  /data/users/dhananjay/nlp244/quest_4/data/french/train\n",
      "File doesn't exist : /data/users/dhananjay/nlp244/quest_4/data/french/validation\n",
      "creating file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]/data/users/dhananjay/miniconda3/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████| 20/20 [01:59<00:00,  5.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created!\n",
      "Successfully loaded file -  /data/users/dhananjay/nlp244/quest_4/data/french/validation\n",
      "File doesn't exist : /data/users/dhananjay/nlp244/quest_4/data/french/test\n",
      "creating file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [01:58<00:00,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created!\n",
      "Successfully loaded file -  /data/users/dhananjay/nlp244/quest_4/data/french/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "french_train_dataset = load_french_data(english_train_datloader, t5small_model, t5small_tokenizer, french_train_filepath)\n",
    "french_val_dataset = load_french_data(english_val_dataloader, t5small_model, t5small_tokenizer, french_validation_filepath)\n",
    "french_test_dataset = load_french_data(english_test_dataloader, t5small_model, t5small_tokenizer, french_test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "202d6d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train, validation, test dataset\n",
    "final_data = DatasetDict()\n",
    "final_data['train'] = french_train_dataset\n",
    "final_data['validation'] = french_val_dataset\n",
    "final_data['test'] = french_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "832b075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(final_filepath):\n",
    "    with open(final_filepath, \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(final_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4fa94b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /soe/dsonawan/.huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# push transalted data to the huggingface\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e8a1b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026015043258666992,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Pushing dataset shards to the dataset hub",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802c4921d00f4aa48f9411257dd8d204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.024710416793823242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Creating parquet from Arrow format",
       "rate": null,
       "total": 100,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf4a08f34b146dd97b5fc1be143c7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split validation to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.025499343872070312,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Pushing dataset shards to the dataset hub",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc7ce5089f94901a975dd0ec7959940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.025049209594726562,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Creating parquet from Arrow format",
       "rate": null,
       "total": 10,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90855d85e560492ebc52b792de219d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split test to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020103931427001953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Pushing dataset shards to the dataset hub",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94fb75898bd4139990cae4700531495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016895771026611328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Creating parquet from Arrow format",
       "rate": null,
       "total": 10,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b2aef0cabb4019b9b2df765b3d361c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_data.push_to_hub(\"dhananjay1210/SNLI_French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c244ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
